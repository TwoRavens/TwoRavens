{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, math, json\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## one-hot-encoding of categorical features\n",
    "def encode_onehot(df, cols):\n",
    "    \"\"\"\n",
    "    One-hot encoding is applied to columns specified in a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    vec = DictVectorizer()\n",
    "    vec_data = pd.DataFrame(vec.fit_transform(df[cols].to_dict(orient='records')).toarray())\n",
    "    vec_data.columns = vec.get_feature_names()\n",
    "    vec_data.index = df.index\n",
    "    \n",
    "    df = df.drop(cols, axis=1)\n",
    "    df = df.join(vec_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#projectdir = os.path.realpath(__file__).split('src')[0]\n",
    "projectDir = \"../../\"\n",
    "solDir = os.path.join(projectDir, \"solution\")\n",
    "dataDir = os.path.join(projectDir, \"data\")\n",
    "assert os.path.exists(solDir)\n",
    "assert os.path.exists(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in training data ....\n"
     ]
    }
   ],
   "source": [
    "print('loading in training data ....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData = pd.read_csv(os.path.join(dataDir, \"trainData.csv.gz\"), sep=',', compression='gzip', index_col=0)\n",
    "trainTargets = pd.read_csv(os.path.join(dataDir, \"trainTargets.csv.gz\"), sep=',', compression='gzip', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-assign training data and targets for scikit-learn\n",
    "X_train = trainData\n",
    "y_train = trainTargets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding derived word length feature ....\n"
     ]
    }
   ],
   "source": [
    "# add word-length feature\n",
    "print('adding derived word length feature ....')\n",
    "X_train['word_length'] = X_train.apply(lambda row: len(row['Word']),axis=1)\n",
    "# quick and dirty\n",
    "X_train.drop(['Word'],axis=1, inplace=True)\n",
    "# One-Hot Encode Categorical Variables\n",
    "# simple categorical column detection\n",
    "cat_cols = []\n",
    "for index,val in X_train.tail(1).iteritems():\n",
    "    if isinstance(val.values[0],str): # simple categorical feature detection\n",
    "        cat_cols.append(index)\n",
    "# one-hot encode\n",
    "X_train = encode_onehot(X_train, cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model on train data ...\n",
      "using 10-fold CV...\n",
      "f1 score on training CV 0.317546603623\n"
     ]
    }
   ],
   "source": [
    "print('training model on train data ...')\n",
    "print('using 10-fold CV...')\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "parameters = {'n_estimators':[100, 200, 300]}\n",
    "rf = GridSearchCV(estimator=ensemble.RandomForestClassifier(class_weight=\"balanced\"), \n",
    "                       param_grid=parameters, \n",
    "                       scoring='f1_macro', \n",
    "                       cv=cv)\n",
    "# rf = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train,y_train['Level.Teachers.Average'])\n",
    "# print(list(zip(X_train, rf.best_estimator_.feature_importances_)))\n",
    "rf_score = rf.best_score_\n",
    "print('f1 score on training CV', rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making prediciton on test data ....\n",
      "loading test data ...\n",
      "calling predict ...\n",
      "saving predictions to testTargets.csv ...\n"
     ]
    }
   ],
   "source": [
    "print('making prediciton on test data ....')\n",
    "print('loading test data ...')\n",
    "if os.path.exists(os.path.join(dataDir, \"testData.csv.gz\")):\n",
    "    testData = pd.read_csv(os.path.join(dataDir, \"testData.csv.gz\"), sep=',', compression='gzip', index_col=0)\n",
    "    X_test = testData\n",
    "    # add word-length feature\n",
    "    X_test['word_length'] = X_test.apply(lambda row: len(row['Word']),axis=1)\n",
    "    # quick and dirty\n",
    "    X_test.drop(['Word'],axis=1, inplace=True)\n",
    "    # One-Hot Encode Categorical Variables\n",
    "    # simple categorical column detection\n",
    "    cat_cols = []\n",
    "    for index,val in X_test.tail(1).iteritems():\n",
    "        if isinstance(val.values[0],str): # simple categorical feature detection\n",
    "            cat_cols.append(index)\n",
    "    # one-hot encode\n",
    "    X_test = encode_onehot(X_test, cat_cols)\n",
    "    print('calling predict ...')\n",
    "    y_pred = pd.DataFrame(rf.predict(X_test))\n",
    "    y_pred.columns = y_train.columns\n",
    "    y_pred.index.name = y_train.index.name\n",
    "    print('saving predictions to testTargets.csv ...')\n",
    "    y_pred.to_csv('testTargets.csv')\n",
    "else:\n",
    "    print('looks like this is a redacted dataset. This step cannot be completed ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing performance on test data ...\n",
      "f1 score on test data 0.339578993394\n"
     ]
    }
   ],
   "source": [
    "print('computing performance on test data ...')\n",
    "if os.path.exists(os.path.join(dataDir, \"testTargets.csv.gz\")):\n",
    "    testTargets = pd.read_csv(os.path.join(dataDir, \"testTargets.csv.gz\"), sep=',', compression='gzip', index_col=0)\n",
    "    y_test = testTargets\n",
    "    y_pred = pd.read_csv('testTargets.csv')\n",
    "    conf_mat = metrics.confusion_matrix(y_test['Level.Teachers.Average'], y_pred['Level.Teachers.Average'])\n",
    "    f1 = metrics.f1_score(y_test['Level.Teachers.Average'], y_pred['Level.Teachers.Average'], average='macro')\n",
    "    print('f1 score on test data', f1)\n",
    "else:\n",
    "    print('looks like this is a redacted dataset. This step cannot be completed ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 57,  56,  28,   6,   2,   4,  29],\n",
       "       [ 37,  91,  81,  22,   9,  10,  31],\n",
       "       [ 17,  82, 143,  47,  13,  21,  24],\n",
       "       [  7,  27,  68,  46,  15,  29,  57],\n",
       "       [  0,   4,  12,   9,  13,  19,  35],\n",
       "       [  0,   0,   0,   1,   4,  14,  19],\n",
       "       [  2,   0,   0,   1,   6,  11, 191]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving performance.json ...\n",
      "{\n",
      "  \"train\": {\n",
      "    \"split\": {\n",
      "      \"type\": \"StratifiedKFold\",\n",
      "      \"n_splits\": 10,\n",
      "      \"random_state\": 42,\n",
      "      \"shuffle\": true\n",
      "    },\n",
      "    \"score\": {\n",
      "      \"metric\": \"f1Macro\",\n",
      "      \"value\": 0.31754660362282333\n",
      "    }\n",
      "  },\n",
      "  \"test\": {\n",
      "    \"score\": {\n",
      "      \"metric\": \"f1Macro\",\n",
      "      \"value\": 0.33957899339438263\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "train_performance = OrderedDict([\n",
    "        ('train', OrderedDict([\n",
    "            ('split', OrderedDict([\n",
    "                    ('type', 'StratifiedKFold'),\n",
    "                    ('n_splits', 10),\n",
    "                    ('random_state', 42),\n",
    "                    ('shuffle', True)\n",
    "                    ])\n",
    "            ),\n",
    "            ('score', OrderedDict([\n",
    "                    ('metric', 'f1Macro'),\n",
    "                    ('value', rf_score)])\n",
    "            )\n",
    "        ]))\n",
    "    ])\n",
    "\n",
    "\n",
    "test_performance = OrderedDict([\n",
    "        ('test', OrderedDict([\n",
    "            ('score', OrderedDict([\n",
    "                    ('metric', 'f1Macro'),\n",
    "                    ('value', f1)])\n",
    "            )\n",
    "        ]))\n",
    "    ])\n",
    "print('saving performance.json ...')\n",
    "\n",
    "overall_performance = OrderedDict()\n",
    "overall_performance.update(train_performance)\n",
    "overall_performance.update(test_performance)\n",
    "\n",
    "with open('performance.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(overall_performance, f, indent=2)\n",
    "print(json.dumps(overall_performance, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:convscripts]",
   "language": "python",
   "name": "conda-env-convscripts-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
