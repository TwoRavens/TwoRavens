{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svattam/.envs/jupyter/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import nltk, os, glob\n",
    "import pandas as pd\n",
    "from normalization import normalize_corpus, tokenize_text\n",
    "import numpy as np\n",
    "import codecs\n",
    "from sklearn.datasets.base import Bunch\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit, KFold\n",
    "from feature_extractors import bow_extractor, tfidf_extractor\n",
    "from feature_extractors import averaged_word_vectorizer\n",
    "from feature_extractors import tfidf_weighted_averaged_word_vectorizer\n",
    "import nltk\n",
    "import gensim\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import re, json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import OrderedDict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rootDir = '../..'\n",
    "dataDir = os.path.join(rootDir, 'data')\n",
    "assert os.path.exists(dataDir)\n",
    "rawDir = os.path.join(dataDir, 'raw_data')\n",
    "assert os.path.exists(rawDir)\n",
    "\n",
    "TARGET_FIELD = 'extrovert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(whichData='train'):\n",
    "    trainData = pd.read_csv(os.path.join(dataDir, '%sData.csv'%whichData), index_col=0)\n",
    "    trainTargets = pd.read_csv(os.path.join(dataDir, '%sTargets.csv'%whichData), index_col=0)\n",
    "    dataset = Bunch()\n",
    "    dataset.data = np.array([]) \n",
    "    dataset.target = np.array([])\n",
    "    for i, rf in enumerate(trainData['raw_text_file']):\n",
    "        path = os.path.join(rawDir, rf)\n",
    "        raw = open(path, encoding='utf-8').read()\n",
    "        dataset.data = np.append(dataset.data, raw)\n",
    "    dataset.target = trainTargets[TARGET_FIELD]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading training data corpus ...\n"
     ]
    }
   ],
   "source": [
    "print('reading training data corpus ...')\n",
    "dataset = get_data(whichData='train')\n",
    "corpus, labels = dataset.data, dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizing corpus ...\n"
     ]
    }
   ],
   "source": [
    "print('normalizing corpus ...')\n",
    "norm_corpus = normalize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating BOW features ...\n"
     ]
    }
   ],
   "source": [
    "print('creating BOW features ...')\n",
    "bow_vectorizer, bow_features = bow_extractor(norm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating tfidf features ...\n"
     ]
    }
   ],
   "source": [
    "print('creating tfidf features ...')\n",
    "tfidf_vectorizer, tfidf_features = tfidf_extractor(norm_corpus)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating averaged word vector features ...\n"
     ]
    }
   ],
   "source": [
    "print('creating averaged word vector features ...')\n",
    "tokenized_corpus = [nltk.word_tokenize(text) for text in norm_corpus]\n",
    "model = gensim.models.Word2Vec(tokenized_corpus, size=500, window=100, min_count=30, sample=1e-3)\n",
    "avg_wv_features = averaged_word_vectorizer(corpus=tokenized_corpus, model=model, num_features=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating tfidf weighted averaged word vector features ...\n"
     ]
    }
   ],
   "source": [
    "print('creating tfidf weighted averaged word vector features ...')\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "tfidf_wv_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_corpus, tfidf_vectors=tfidf_features, \n",
    "                                                            tfidf_vocabulary=vocab, \n",
    "                                                            model=model, \n",
    "                                                            num_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing RandomForestClassifier(RFC) and SVM classfiers ...\n"
     ]
    }
   ],
   "source": [
    "print('initializing RandomForestClassifier(RFC) and SVM classfiers ...')\n",
    "rfc = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "svm = SGDClassifier(loss='hinge', n_iter=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[]\n",
    "scores=[]\n",
    "train_performance = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RFC with BOW features ...\n"
     ]
    }
   ],
   "source": [
    "print('training RFC with BOW features ...')\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "cv_scores = cross_val_score(rfc, bow_features, labels, cv=cv, scoring='f1')\n",
    "models.append((rfc, 'bow_features'))\n",
    "scores.append(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training SVM with BOW features ...\n"
     ]
    }
   ],
   "source": [
    "print('training SVM with BOW features ...')\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "cv_scores = cross_val_score(svm, bow_features, labels, cv=cv, scoring='f1')\n",
    "models.append((svm, 'bow_features'))\n",
    "scores.append(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RFC with tfidf features ...\n"
     ]
    }
   ],
   "source": [
    "print('training RFC with tfidf features ...')\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "cv_scores = cross_val_score(rfc, tfidf_features, labels, cv=cv, scoring='f1')\n",
    "models.append((rfc, 'tfidf_features'))\n",
    "scores.append(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training SVM with tfidf features ...\n"
     ]
    }
   ],
   "source": [
    "print('training SVM with tfidf features ...')\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "cv_scores = cross_val_score(svm, tfidf_features, labels, cv=cv, scoring='f1')\n",
    "models.append((svm, 'tfidf_features'))\n",
    "scores.append(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RFC  with avg_wv_features features ...\n"
     ]
    }
   ],
   "source": [
    "print('training RFC  with avg_wv_features features ...')\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "cv_scores = cross_val_score(rfc, avg_wv_features, labels, cv=cv, scoring='f1')\n",
    "models.append((rfc, 'avg_wv_features'))\n",
    "scores.append(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training SVM with avg_wv_features features ...\n"
     ]
    }
   ],
   "source": [
    "print('training SVM with avg_wv_features features ...')\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "cv_scores = cross_val_score(svm, avg_wv_features, labels, cv=cv, scoring='f1')\n",
    "models.append((svm, 'avg_wv_features'))\n",
    "scores.append(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training RFC with tfidf_wv_features features ...\n"
     ]
    }
   ],
   "source": [
    "print('training RFC with tfidf_wv_features features ...')\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "cv_scores = cross_val_score(rfc, tfidf_wv_features, labels, cv=cv, scoring='f1')\n",
    "models.append((rfc, 'tfidf_wv_features'))\n",
    "scores.append(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training SVM with tfidf_wv_features features ...\n"
     ]
    }
   ],
   "source": [
    "print('training SVM with tfidf_wv_features features ...')\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "cv_scores = cross_val_score(svm, tfidf_wv_features, labels, cv=cv, scoring='f1')\n",
    "models.append((svm, 'tfidf_wv_features'))\n",
    "scores.append(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choosing the best model for baseline...\n",
      "baseline model: (SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=100, n_iter=None,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
      "       tol=None, verbose=0, warm_start=False), 'tfidf_features')\n",
      "baseline performance on 10-fold CV (mean f1): 0.628072074984\n"
     ]
    }
   ],
   "source": [
    "print('choosing the best model for baseline...')\n",
    "baseline = models[np.argmax(scores)]\n",
    "baselineScore = scores[np.argmax(scores)]\n",
    "print('baseline model:', str(baseline))\n",
    "print('baseline performance on 10-fold CV (mean f1):', baselineScore)\n",
    "train_performance = OrderedDict([\n",
    "    ('train', OrderedDict([\n",
    "        ('split', OrderedDict([\n",
    "                ('type', 'KFold'),\n",
    "                ('n_splits', 10),\n",
    "                ('shuffle', True),\n",
    "                ('random_state', 0)])\n",
    "        ),\n",
    "        ('score', OrderedDict([\n",
    "                ('metric', 'f1'),\n",
    "                ('value', baselineScore)])\n",
    "        )\n",
    "    ]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training the model on the entire train data...\n"
     ]
    }
   ],
   "source": [
    "print('training the model on the entire train data...')\n",
    "baselineMod = baseline[0]\n",
    "baselineFea = eval(baseline[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=100, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baselineMod.fit(baselineFea, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction on testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making predictions on testData (assuming that testData is available) ...\n",
      "reading test data ...\n",
      "normalizing corpus ...\n",
      "extracting tfidf features ...\n",
      "predicting ...\n",
      "formatting and saving predictions as testTargets.csv ...\n"
     ]
    }
   ],
   "source": [
    "print('making predictions on testData (assuming that testData is available) ...')\n",
    "try:\n",
    "    print('reading test data ...')\n",
    "    dataset = get_data(whichData='test')\n",
    "    corpus, labels = dataset.data, dataset.target\n",
    "\n",
    "    print('normalizing corpus ...')\n",
    "    norm_corpus = normalize_corpus(corpus)\n",
    "    \n",
    "    print('extracting tfidf features ...')\n",
    "    tfidf_features = (tfidf_vectorizer.transform(norm_corpus))\n",
    "    \n",
    "    print('predicting ...')\n",
    "    y_predict = pd.DataFrame(baselineMod.predict(tfidf_features))\n",
    "    \n",
    "    print('formatting and saving predictions as testTargets.csv ...')\n",
    "    y_train = pd.read_csv(os.path.join(dataDir,'trainTargets.csv'), index_col=0)\n",
    "    y_predict.columns = y_train.columns\n",
    "    y_predict.index.name = y_train.index.name\n",
    "    y_predict.to_csv('testTargets.csv')\n",
    "except:\n",
    "    print('Looks like this is a redacted dataset. testData is unavailable. Cannot complete this step ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute perforamnce on testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_performance = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing performance on testData (assuming the testTargets is available) ...\n",
      "reading testTargets...\n",
      "reading predictions ...\n",
      "computing score...\n",
      "performance on test data (f1): 0.666666666667\n",
      "saving the performance score...\n"
     ]
    }
   ],
   "source": [
    "print('computing performance on testData (assuming the testTargets is available) ...')\n",
    "try:\n",
    "    print('reading testTargets...')\n",
    "    y_test = pd.read_csv(os.path.join(dataDir, 'testTargets.csv'), index_col=0)\n",
    "    print('reading predictions ...')\n",
    "    y_predict = pd.read_csv('testTargets.csv', index_col=0)\n",
    "    print('computing score...')\n",
    "    f1 = f1_score(y_test, y_predict)\n",
    "    print('performance on test data (f1):',f1)\n",
    "    print('saving the performance score...')\n",
    "    test_performance = OrderedDict([\n",
    "        ('test', OrderedDict([\n",
    "            ('score', OrderedDict([\n",
    "                    ('metric', 'f1'),\n",
    "                    ('value', f1)])\n",
    "            )\n",
    "        ]))\n",
    "    ])\n",
    "except:\n",
    "    raise\n",
    "#     print('Looks like this is a redacted dataset. testTargets is unavailable. cannot complete this step ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"train\": {\n",
      "    \"split\": {\n",
      "      \"type\": \"KFold\",\n",
      "      \"n_splits\": 10,\n",
      "      \"shuffle\": true,\n",
      "      \"random_state\": 0\n",
      "    },\n",
      "    \"score\": {\n",
      "      \"metric\": \"f1\",\n",
      "      \"value\": 0.6280720749838398\n",
      "    }\n",
      "  },\n",
      "  \"test\": {\n",
      "    \"score\": {\n",
      "      \"metric\": \"f1\",\n",
      "      \"value\": 0.6666666666666666\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "overall_performance = OrderedDict()\n",
    "overall_performance.update(train_performance)\n",
    "overall_performance.update(test_performance)\n",
    "\n",
    "with open('performance.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(overall_performance, f, indent=2)\n",
    "print(json.dumps(overall_performance, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyter]",
   "language": "python",
   "name": "conda-env-jupyter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
