{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svattam/.envs/jupyter/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import nltk, os, glob\n",
    "import pandas as pd\n",
    "from normalization import normalize_corpus, tokenize_text\n",
    "import numpy as np\n",
    "import codecs\n",
    "from sklearn.datasets.base import Bunch\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from feature_extractors import bow_extractor, tfidf_extractor\n",
    "from feature_extractors import averaged_word_vectorizer\n",
    "from feature_extractors import tfidf_weighted_averaged_word_vectorizer\n",
    "import nltk\n",
    "import gensim\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import re, json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rootDir = '../..'\n",
    "dataDir = os.path.join(rootDir, 'data')\n",
    "assert os.path.exists(dataDir)\n",
    "rawDir = os.path.join(dataDir, 'raw_data')\n",
    "assert os.path.exists(rawDir)\n",
    "\n",
    "TARGET_FIELD = 'extrovert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     38,
     44
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(whichData='train'):\n",
    "    trainData = pd.read_csv(os.path.join(dataDir, '%sData.csv'%whichData), index_col=0)\n",
    "    trainTargets = pd.read_csv(os.path.join(dataDir, '%sTargets.csv'%whichData), index_col=0)\n",
    "    dataset = Bunch()\n",
    "    dataset.data = np.array([]) \n",
    "    dataset.target = np.array([])\n",
    "    for i, rf in enumerate(trainData['raw_text_file']):\n",
    "        path = os.path.join(rawDir, rf)\n",
    "        raw = open(path, encoding='utf-8').read()\n",
    "        dataset.data = np.append(dataset.data, raw)\n",
    "    dataset.target = trainTargets[TARGET_FIELD]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def prepare_datasets(corpus, labels, test_data_proportion=0.3):\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels, \n",
    "                                                        test_size=test_data_proportion, random_state=42)\n",
    "    return train_X, test_X, train_Y, test_Y\n",
    "\n",
    "def remove_empty_docs(corpus, labels):\n",
    "    filtered_corpus = []\n",
    "    filtered_labels = []\n",
    "    for doc, label in zip(corpus, labels):\n",
    "        if doc.strip():\n",
    "            filtered_corpus.append(doc)\n",
    "            filtered_labels.append(label)\n",
    "    return filtered_corpus, filtered_labels\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    print('Accuracy:', np.round(\n",
    "                        metrics.accuracy_score(true_labels, predicted_labels),2))\n",
    "    print('Precision:', np.round(\n",
    "                        metrics.precision_score(true_labels, predicted_labels, average='weighted'), 2))\n",
    "    print('Recall:', np.round(\n",
    "                        metrics.recall_score(true_labels, predicted_labels, average='weighted'), 2))\n",
    "    print('F1 Score:', np.round(\n",
    "                        metrics.f1_score(true_labels, predicted_labels, average='weighted'), 2))\n",
    "    \n",
    "\n",
    "def train_model(classifier, train_features, train_labels):\n",
    "    # build model    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    return classifier\n",
    "    \n",
    "\n",
    "def train_predict_evaluate_model(classifier, \n",
    "                                 train_features, train_labels, \n",
    "                                 test_features, test_labels):\n",
    "    # build model    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features) \n",
    "    # evaluate model prediction performance   \n",
    "    get_metrics(true_labels=test_labels, \n",
    "                predicted_labels=predictions)\n",
    "    return classifier, predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model space exploration on testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploring model space on train data...\n"
     ]
    }
   ],
   "source": [
    "print('exploring model space on train data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_data(whichData='train')\n",
    "corpus, labels = dataset.data, dataset.target\n",
    "# len(corpus)\n",
    "# print(len(labels))\n",
    "# print(corpus[0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, labels = remove_empty_docs(corpus, labels)\n",
    "# print(corpus[0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split the training data into train and test corpus...\n"
     ]
    }
   ],
   "source": [
    "print('split the training data into train and test corpus...')\n",
    "train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(corpus,labels,test_data_proportion=0.3)\n",
    "# print(train_corpus[0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizing the corpus...\n"
     ]
    }
   ],
   "source": [
    "# normalize the corpus\n",
    "print('normalizing the corpus...')\n",
    "norm_train_corpus = normalize_corpus(train_corpus)\n",
    "norm_test_corpus = normalize_corpus(test_corpus)\n",
    "# print(norm_train_corpus[0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW feature extraction...\n"
     ]
    }
   ],
   "source": [
    "# bag of words features\n",
    "print('BOW feature extraction...')\n",
    "bow_vectorizer, bow_train_features = bow_extractor(norm_train_corpus)  \n",
    "bow_test_features = bow_vectorizer.transform(norm_test_corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF feature extraction...\n"
     ]
    }
   ],
   "source": [
    "# tfidf features\n",
    "print('TFIDF feature extraction...')\n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(norm_train_corpus)  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize documents\n",
    "tokenized_train = [nltk.word_tokenize(text) for text in norm_train_corpus]\n",
    "tokenized_test = [nltk.word_tokenize(text) for text in norm_test_corpus]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building word2vec model...\n"
     ]
    }
   ],
   "source": [
    "# build word2vec model\n",
    "print('building word2vec model...')\n",
    "model = gensim.models.Word2Vec(tokenized_train, size=500, window=100, min_count=30, sample=1e-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "averaged word vector features...\n"
     ]
    }
   ],
   "source": [
    "print('averaged word vector features...')\n",
    "# averaged word vector features\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train,\n",
    "                                                 model=model,\n",
    "                                                 num_features=500)                   \n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test,\n",
    "                                                model=model,\n",
    "                                                num_features=500)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf weighted averaged word vector features...\n"
     ]
    }
   ],
   "source": [
    "print('tfidf weighted averaged word vector features...')\n",
    "# tfidf weighted averaged word vector features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_train, \n",
    "                                                                  tfidf_vectors=tfidf_train_features, \n",
    "                                                                  tfidf_vocabulary=vocab, \n",
    "                                                                  model=model, \n",
    "                                                                  num_features=500)\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_test, \n",
    "                                                                 tfidf_vectors=tfidf_test_features, \n",
    "                                                                 tfidf_vocabulary=vocab, \n",
    "                                                                 model=model, \n",
    "                                                                 num_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing two kinds of classifiers: MNB and SVM for trying various types of features...\n",
      "performance on taining data:\n",
      "Accuracy: 0.46\n",
      "Precision: 0.76\n",
      "Recall: 0.46\n",
      "F1 Score: 0.32\n",
      "\n",
      "Accuracy: 0.46\n",
      "Precision: 0.76\n",
      "Recall: 0.46\n",
      "F1 Score: 0.32\n",
      "\n",
      "Accuracy: 0.43\n",
      "Precision: 0.18\n",
      "Recall: 0.43\n",
      "F1 Score: 0.26\n",
      "\n",
      "Accuracy: 0.54\n",
      "Precision: 0.68\n",
      "Recall: 0.54\n",
      "F1 Score: 0.49\n",
      "\n",
      "Accuracy: 0.57\n",
      "Precision: 0.33\n",
      "Recall: 0.57\n",
      "F1 Score: 0.42\n",
      "\n",
      "Accuracy: 0.69\n",
      "Precision: 0.69\n",
      "Recall: 0.69\n",
      "F1 Score: 0.67\n"
     ]
    }
   ],
   "source": [
    "print('initializing two kinds of classifiers: MNB and SVM for trying various types of features...')\n",
    "print('performance on taining data:')\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', n_iter=100, random_state=42)\n",
    "\n",
    "# Multinomial Naive Bayes with bag of words features\n",
    "(mnb_bow_model, mnb_bow_predictions) = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "print()\n",
    "# Support Vector Machine with bag of words features\n",
    "(svm_bow_model, svm_bow_predictions) = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "print()                                    \n",
    "# Multinomial Naive Bayes with tfidf features                                           \n",
    "(mnb_tfidf_model, mnb_tfidf_predictions) = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "print()\n",
    "# Support Vector Machine with tfidf features\n",
    "(svm_tfidf_model, svm_tfidf_predictions) = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "print()\n",
    "# Support Vector Machine with averaged word vector features\n",
    "(svm_avgwv_model, svm_avgwv_predictions) = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "print()\n",
    "# Support Vector Machine with tfidf weighted averaged word vector features\n",
    "(svm_tfidfwv_model, svm_tfidfwv_predictions) = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_wv_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "cm = metrics.confusion_matrix(test_labels, svm_bow_predictions)\n",
    "# print(pd.DataFrame(cm, index=range(0,2), columns=range(0,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we select svm_bow_model after exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train svm_bow_model on full trainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected SVM_BOW model as it performed the best. Training that model on full train data...\n"
     ]
    }
   ],
   "source": [
    "print('Selected SVM_BOW model as it performed the best. Training that model on full train data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train the model on the whole training corpus\n",
    "dataset = get_data(whichData='train')\n",
    "corpus, labels = dataset.data, dataset.target\n",
    "corpus, labels = remove_empty_docs(corpus, labels)\n",
    "norm_corpus = normalize_corpus(corpus)\n",
    "bow_vectorizer, bow_features = bow_extractor(norm_corpus)  \n",
    "svm_bow_model = train_model(classifier=svm,train_features=bow_features, train_labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit predictions on testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making predictions on test data...(will throw exception on redacted dataset as testData.csv is unavailable)\n"
     ]
    }
   ],
   "source": [
    "print('making predictions on test data...(will throw exception on redacted dataset as testData.csv is unavailable)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = pd.read_csv(os.path.join(dataDir, 'testData.csv'), index_col=0)\n",
    "# print(testDF.shape)\n",
    "# print(testDF.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdataset = get_data(whichData='test')\n",
    "testcorpus = testdataset.data\n",
    "norm_test_corpus = normalize_corpus(testcorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter words in test corpus that are not contained in the training vocabulary\n",
    "filtered_corpus = []    \n",
    "for text in norm_test_corpus:\n",
    "    filtered_tokens = []\n",
    "    tokens = tokenize_text(text)\n",
    "    for token in tokens:\n",
    "        if token in bow_vectorizer.vocabulary_:\n",
    "            filtered_tokens.append(token)\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    filtered_corpus.append(filtered_text)\n",
    "# print(len(filtered_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_test_features = bow_vectorizer.transform(filtered_corpus) \n",
    "df = pd.DataFrame(svm_bow_model.predict(bow_test_features), index=testDF.index)\n",
    "df.columns = [TARGET_FIELD]\n",
    "df.to_csv('testTargets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute performance on testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing performance on test data...(will throw exception on redacted dataset as testTargets.csv is unavailable)\n"
     ]
    }
   ],
   "source": [
    "print('computing performance on test data...(will throw exception on redacted dataset as testTargets.csv is unavailable)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predict = pd.read_csv('testTargets.csv', index_col=0)\n",
    "y_truth = pd.read_csv(os.path.join(dataDir,'testTargets.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_predict.head())\n",
    "# print(y_truth.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.round(metrics.f1_score(y_truth, y_predict, average='weighted'), 4)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance on test data...\n",
      "Accuracy: 0.62\n",
      "Precision: 0.62\n",
      "Recall: 0.62\n",
      "F1 Score: 0.62\n"
     ]
    }
   ],
   "source": [
    "print('performance on test data...')\n",
    "print('Accuracy:', np.round(metrics.accuracy_score(y_truth, y_predict),2))\n",
    "print('Precision:', np.round(metrics.precision_score(y_truth, y_predict, average='weighted'), 2))\n",
    "print('Recall:', np.round(metrics.recall_score(y_truth, y_predict, average='weighted'), 2))\n",
    "print('F1 Score:', np.round(metrics.f1_score(y_truth, y_predict, average='weighted'), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the score\n",
    "score = {'f1':score}\n",
    "# write to score file\n",
    "with open('performance.json', 'w') as outfile:\n",
    "    json.dump(score, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyter]",
   "language": "python",
   "name": "conda-env-jupyter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
